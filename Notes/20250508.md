### ChatGPT

- 它只是一次添加一个词。
  - LLM：large language model，大语言模型
  - ChatGPT 始终要做的是，针对它得到的任何文本信息产生“合理的延续”。
  - “根据目前的文本，下一个词应该是什么”，ChatGPT每次都是添加一个“标记（token）”。--有可能造词
  - 下一个token的选择是从一个带有概率的词列表中选择，如果每次选择概率最高的词可能会得到一个平淡的文章。因此我们需要引入一些随机性——“温度”参数。
- 概率从何而来
  - `n` 元（`n-gram`）字母，n 元词：不只是关注单个字母或者单个词出现的概率，而是考虑单个词出现之后 n 个词出现的概率，但是对于4万个常用词，二元词的组合是16亿，三元词的组合是64万亿，很显然无法把这个持续做下去。
  - 最佳的思路则是建立一个模型，让模型来估计序列出现的概率
- 什么是模型
  - 建立一个模型，用它提供某种计算答案的程序，而不仅仅是在每种情况下测量和记录。
  - 从来没有“无模型的模型”，使用的任何模型都有某种特定的基本结构，以及用于拟合数据的一定数量的“旋钮”（可以设置的参数）
- 类人任务（human-like task）的模型
  - 数值数据建模 vs 人类语言文本建模
  - 类人任务：图像识别，{一个图像的像素值集合} → function → 一个数字
  - 我们的目标是为人类在识别图像方面的能力生成一个模型，真正需要问的问题是：面对一个模糊的图像，并且不知道其来源，人类会用什么方式来识别它？如果函数给出的结果总是与人类的意见相符，那么我们就有一个“好模型”。
    - 对于图像识别这样的任务，我们现在基本上已经知道如何构建不错的函数了。
  - 能“用数学证明”这些函数有效么？**不能**，因为我们没有一个关于人类所做的事情的数学理论。



-----



### 神经网络

> 以图像识别为例讲解神经网络，即如何识别图片中的 `1, 2, 3,...`

- 图像识别等任务的典型模型，最受欢迎且最成功的方法是使用神经网络。
- 神经网络发明于20世纪40年代，视作对大脑工作机制的简单理想化。
  - 人类大脑有约1000亿个神经元，每个神经元都能产生电脉冲。
  - 这些神经元连接成复杂的网络，每个神经元都有树枝状的分支，从而能够向其他数以千计的神经元传递电信号。
  - 任意一个神经元在某个时刻是否产生电脉冲，取决于它从其他神经元接收到的电脉冲，而且神经元不同的连接方式会有不同的“权重”贡献。
- 神经网络其实没有明确的“理论解释”，它是作为一项工程在1998年被构建出来，而且被发现可以奏效。
- **吸引子（attractor）**
  - 我们希望通过某种方法将所有的1“吸引到一个地方”，将所有的2“吸引到另外一个地方”，如果一个图像“更有可能是1”而不是2，我们希望他最终出现在“1的地方”。
  - 假设平面上存在一些点，我们从平面上的任意一点出发，并且总是希望最终到达最近的点。可以用理想化的“分水岭”将平面分割成不同的区域（“吸引子盆地”）。
  - 我们可以将这看成是执行一种“识别任务”，所做的不是识别一个给定图像“看起来最像”哪个数字，而是相当直接地看出哪个点距离给定的点最近。
- 如何让神经网络“执行识别任务”？
  - 神经网络是由理想化的“神经元”组成的连接集合——通常是按层排列的。
  - 每个神经元都被有效地设置为计算一个简单的数值函数。
  - 为了使用这个网络，我们只需要在顶部输入一些数
  - 然后让每层神经元“计算它们的函数的值”，并在网格中将结果前馈，最后在底部产生最终结果。
  - 每个神经元实际上都有一些来自前一层神经元的“输入连接”，而且每个连接都被分配了一个特定的“权重”（正或者负）。
  - 给定神经元的值是这样确定的：先分别将其“前一层神经元”的值乘以相应的权重并将结果相加，然后加上一个常数，最后应用一个“阈值”函数（或者称为“激活”函数）。
    - 如果神经元的输入是 $\overrightarrow{x} = \begin{Bmatrix} x_1, x_2, ... \end{Bmatrix}$ ，那么我们需要计算 $f[\overrightarrow{w}\overrightarrow{x} + b]$，权重w和常量b对于网格中的每个神经元选择不同的值；函数f通常在所有神经元中保持不变。
    - 激活函数f使用非线性函数
  - 我们希望神经网络执行的每个任务，或者说，对于我们希望它计算的每个整体函数，都有不同的权重选择。（这些权重通常是利用机器学习根据我们想要的输出的示例“训练”神经网络来确定的。）
  - 最终，每个神经网络都只对于某个整数的数学函数。同样，对于ChatGPT的神经网络也只对应于一个这样的数学函数——尽管它的项是以亿为单位的。
- 更大的神经网络通常能够更好地逼近我们所求的函数。
- 在“每个吸引子盆地的中心”，通常能确切的得到想要的答案，但是在边界处，也就是神经网络“很难下定决心”的地方，情况可能会更加混乱。
- 我们能“从数学上”解释网络是如何做出区分的么？并不能，它只是在“做神经网络要做的事”，但是事实证明，这通常与我们人类所做的区分相当吻合。
- 无论输入是什么，神经网络都会生成一个答案。结果表明，它的做法相当符合人类的思维方式，这并不是我们可以“根据第一性原则推导”出来的事实。只是一些经验性的发现，至少在某些领域是正确的，这是神经网络有用的一个关键原因：它们以某种方式捕捉了“类似人类”的做事方式。
- 我们可以说神经网络正在“挑选出某些特征”，并使用这些特征来确定图像的内容，但是这些特征能否用语言描述出来呢？大多数情况下不能。
- 但至少到目前为止，我们没办法对神经网络正在做什么“给出语言描述”。

-----

### 机器学习和神经网络的训练

- 神经网络之所以很有用，原因不仅在于它可以执行各种任务，还在于它可以通过逐步“根据样例训练”来学习执行这些任务。
- 不需要编写程序明确指明特征，只需要展示样例，让神经网络从中机器学习如何区分它们即可。
- “泛化”
- 神经网络的训练究竟是如何起效的呢？本质上，我们一直在尝试找到能使神经网络成功复现给定样例的权重。然后，我们依靠神经网络在这些样例之间进行合理的插值（或泛化）。
- 如何才能找到能够复现函数的权重呢？
  - 基本思想是提供大量的“输入→输出”样例以供学习，然后尝试找到能够复现这些样例的权重。
  - 在训练的每个阶段，会逐步调整神经网络的权重，最终我们会得到一个能够成功复现我们想要的函数的神经网络。
  - 应该如何调整权重呢？基本思想是，在每个阶段看一下我们离想要的函数“有多远”，然后朝更接近函数的方向更新权重。
  - 为了明白离目标有多远，就需要计算损失函数，常用的损失函数如欧几里得距离
  - 如何调整权重以减少损失函数？损失函数给出了我们得到的值和真实值之间的“距离”。但是“我们得到的值”在每个阶段是由神经网络的当前版本和其中的权重确定的。现在假设权重是变量，比如 。我们想找出如何调整这些变量的值，以最小化取决于它们的损失。
  - 梯度下降。

----

### 神经网络训练的艺术

- 针对特定的任务使用何种神经网络架构的问题
- 如何获取用于训练神经网络的数据

一个新的神经网络可以直接包含另外一个已经训练过的网络，或者至少可以使用该网络为自己生成更多的训练样例。

做尽可能少的事情 → 解决端对端的问题

将复杂的独立组件引入神经网络，让它有效地“显式实现特定的算法思想” → 只处理非常简单的组件，并让它们“自我组织”来实现（可能）等效的算法思想。

神经网络的一个重要特征是，它们说到底只是在处理数据——和计算机一样。

- 如何确定特定的任务需要多大的神经网络呢？艺术！
  - 任务有多难

假设我们已经确定了一种特定的神经网络架构，现在的问题是如何获取用于训练网络的数据。

- 神经网络以及广义的机器学习的许多实际挑战几种在获取或准备必要的训练数据上。
- 在许多情况（监督学习）下，需要获取明确的输入样例和期望的输出。
- 为特定的任务训练神经网络需要多少数据？根据第一性原则很难估计。
  - 使用迁移学习可以将已经在另一个神经网络中学习到的重要特征列表迁移过来，从而显著减低对数据规模的要求。
  - 但是神经网络通常需要“看到很多样例”才能训练好。
  - 样例的重复。事实上，不断地向神经网络展示所有的样例是一种标准策略。在每个“训练轮次”（training round 或 epoch）中，神经网络都会处于至少稍微不同的状态，而且向它“提醒”某个特定的样例对于它“记忆该样例”是有用的。（是的，这或许类似于重复在人类记忆中的有用性。）
  - 仅仅不断重复相同的样例并不够，还需要向神经网络展示样例的变化。神经网络学问的一个特点是，这些数据增强的变化并不一定要很复杂才有用。

GhatGPT可以进行“无监督学习”，这样更容易获取训练样例。

我们可以应用机器学习来自动化机器学习，并自动设置超参数等。

神经网络的基本思想是利用大量简单（本质上相同）的组件来创建一个灵活的“计算结构”，并使其能够逐步通过学习样例得到改进。

神经网络的训练目前基本上是顺序进行的，每批样例的影响都会被反向传播以更新权重。



-------



### 足够大的神经网络当然无所不能

计算不可约性，意味着我们永远不能保证意外不会发生一只有通过明确的计算，才能知道在任何特定的情况下会升级发生什么。















